---
title: "Project_Soccer"
author: "Namit Shrivastava"
format: pdf
editor: visual
---

## Data Collection

```{r}
library(RedditExtractoR)
library(worldfootballR)
library(tidyverse)
library(httr)
library(jsonlite)

# 1: Collecting data from Reddit
# =========================================

# 1.1 Now let me collect player data from fbref.com using worldfootballR
# Getting data from the 2019-2020, 2020-2021, 2021-2022, and 2022-2023 seasons
player_seasons <- c("2019-2020", "2020-2021", "2021-2022", "2022-2023")

# Initialize empty data frames for each type of statistics
player_standard_stats <- data.frame()
player_shooting_stats <- data.frame()
player_passing_stats <- data.frame()
player_defensive_stats <- data.frame()
player_possession_stats <- data.frame()

# Looping through seasons to collect data
for (season in player_seasons) {
  cat("Collecting data for season:", season, "\n")
  season_end_year <- as.numeric(substr(season, 6, 9))
  
  # Checking the available functions and parameters
  help(fb_big5_advanced_season_stats)
  
  # First I will get all big 5 leagues data, 
  # then filter for Premier League
  
  # Collecting standard statistics
  cat("Collecting standard statistics...\n")
  temp_standard <- fb_big5_advanced_season_stats(season_end_year = season_end_year,
                                               stat_type = "standard",
                                               team_or_player = "player")
  temp_standard <- temp_standard %>% filter(grepl("Premier League", Comp))
  player_standard_stats <- bind_rows(player_standard_stats, temp_standard)
  Sys.sleep(2)
  
  # Collecting shooting statistics
  cat("Collecting shooting statistics...\n")
  temp_shooting <- fb_big5_advanced_season_stats(season_end_year = season_end_year,
                                               stat_type = "shooting",
                                               team_or_player = "player")
  temp_shooting <- temp_shooting %>% filter(grepl("Premier League", Comp))
  player_shooting_stats <- bind_rows(player_shooting_stats, temp_shooting)
  Sys.sleep(2)
  
  # Collecting passing statistics
  cat("Collecting passing statistics...\n")
  temp_passing <- fb_big5_advanced_season_stats(season_end_year = season_end_year,
                                              stat_type = "passing",
                                              team_or_player = "player")
  temp_passing <- temp_passing %>% filter(grepl("Premier League", Comp))
  player_passing_stats <- bind_rows(player_passing_stats, temp_passing)
  Sys.sleep(2)
  
  # Collecting defensive statistics
  cat("Collecting defensive statistics...\n")
  temp_defensive <- fb_big5_advanced_season_stats(season_end_year = season_end_year,
                                                stat_type = "defense",
                                                team_or_player = "player")
  temp_defensive <- temp_defensive %>% filter(grepl("Premier League", Comp))
  player_defensive_stats <- bind_rows(player_defensive_stats, temp_defensive)
  Sys.sleep(2)
  
  # Collecting possession statistics
  cat("Collecting possession statistics...\n")
  temp_possession <- fb_big5_advanced_season_stats(season_end_year = season_end_year,
                                                 stat_type = "possession",
                                                 team_or_player = "player")
  temp_possession <- temp_possession %>% filter(grepl("Premier League", Comp))
  player_possession_stats <- bind_rows(player_possession_stats, temp_possession)
  

  Sys.sleep(5)
  cat("Completed collecting data for", season, "\n\n")
}
```

Ok so when building this dataset, my primary goal was to create a comprehensive player performance repository that could reveal hidden patterns in soccer analytics. I focused on four consecutive Premier League seasons (2019-2023) to capture both pre- and post-pandemic performance trends.

Interestingly, filtering specifically for Premier League data from the broader "Big Five" European leagues helped maintain focus on England's top-tier competition.

So this granular dataset helps me answer critical questions like:

* Can we predict how a creative midfielder might perform in a counter-attacking system versus possession-based tactics? 
* How do aging curves differ between center-backs and wingers? 
* By connecting these performance fingerprints to team strategies and transfer outcomes, the analysis could fundamentally change how clubs approach squad building and in-game decision-making.


## Data Exploration

```{r}
# 2: Data Exploration and Structuring
# ===================================

# 2.1 Exploring the structure of collected data
cat("Dimensions of collected datasets:\n")
cat("Standard stats:", dim(player_standard_stats), "\n")
cat("Shooting stats:", dim(player_shooting_stats), "\n")
cat("Passing stats:", dim(player_passing_stats), "\n")
cat("Defensive stats:", dim(player_defensive_stats), "\n")
cat("Possession stats:", dim(player_possession_stats), "\n\n")

# 2.2 Display column names for understanding the data structure
cat("Standard stats columns:\n")
names(player_standard_stats)[1:20] # Show first 20 columns

# First, checking the column names to find the right minutes column name
colnames(player_standard_stats)

# Creating a summary table of available players by season with corrected column names
player_season_counts <- player_standard_stats %>%
  group_by(Season = as.character(Season_End_Year)) %>%
  summarize(
    Player_Count = n_distinct(Player),
    Teams = n_distinct(Squad),
    # Trying different possible column names for minutes played
    Total_Minutes = sum(if("Min" %in% names(.)) Min else if("Mins" %in% names(.)) Mins else if("MP" %in% names(.)) MP else 0, na.rm = TRUE),
    Goals = sum(if("Gls" %in% names(.)) Gls else if("G" %in% names(.)) G else if("Goals" %in% names(.)) Goals else 0, na.rm = TRUE),
    Assists = sum(if("Ast" %in% names(.)) Ast else if("A" %in% names(.)) A else if("Assists" %in% names(.)) Assists else 0, na.rm = TRUE)
  )

# Displaying the summary
knitr::kable(player_season_counts, 
             caption = "Premier League Players by Season",
             format = "simple")
```


When analyzing the results, I noticed an unexpected issue that the total minutes played across all seasons showed as zero. Initially, this was puzzling, especially since the dataset successfully captured over 500 unique players per season, along with consistent team counts (20 teams per season) and stable offensive metrics like goals and assists. For instance, the number of goals ranged from 986 to 1039 annually, while assists varied between 685 and 743. These figures aligned well with expectations for Premier League seasons and indicated that the attacking data was intact.

Upon closer inspection, I realized the issue stemmed from inconsistent column naming conventions across different datasets. While some tables used "Min" to represent minutes played, others used variations like "Mins," "MP," or even "Min_Playing." My conditional logic for column selection failed to account for these discrepancies, leading to the zero totals for minutes played.

So the gradual increase in player counts from 515 in the 2019-2020 season to 554 in the 2022-2023 season also caught my attention. This growth suggests greater squad rotation or increased utilization of younger players over time, possibly influenced by pandemic-related fixture congestion or evolving team strategies. Despite this turnover, the stability in goals and assists totals across seasons highlights a consistent league-wide offensive output.
To address the missing minutes data, I revisited the raw files and implemented a unified column renaming step to standardize key variables across datasets. This adjustment not only resolved the issue but also underscored the importance of thorough data preprocessing when working with diverse sources. Moving forward, I plan to explore whether offensive production is concentrated among a few star players or distributed evenly across squads












## Data Collection from Reddit

```{r}
library(RedditExtractoR)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(ggrepel)

# Function to safely execute Reddit API calls with rate limiting
safe_reddit_call <- function(func, ..., delay = 5, max_attempts = 3) {
  for (attempt in 1:max_attempts) {
    tryCatch({
      # Execute the function
      result <- func(...)
      
      # If successful, add delay and return result
      cat(paste("Successfully executed Reddit API call. Waiting", delay, "seconds before next call...\n"))
      Sys.sleep(delay)
      return(result)
    }, error = function(e) {
      cat(paste("Attempt", attempt, "failed with error:", e$message, "\n"))
      
      if (grepl("429|rate limit", e$message, ignore.case = TRUE)) {
        # Rate limit hit - wait longer
        wait_time <- delay * 3 * attempt
        cat(paste("Rate limit likely hit. Waiting", wait_time, "seconds before retry...\n"))
        Sys.sleep(wait_time)
      } else {
        # Other error - wait standard time
        cat(paste("Waiting", delay, "seconds before retry...\n"))
        Sys.sleep(delay)
      }
      
      if (attempt == max_attempts) {
        cat("Maximum attempts reached. Returning NULL.\n")
        return(NULL)
      }
    })
  }
}
```
```{r}
# 2. Find Subreddits (Existing Code)
cat("Searching for Premier League subreddits...\n")
premier_league_subreddits <- safe_reddit_call(
  find_subreddits, 
  keywords = "Premier League", 
  delay = 3
)

# 3. Get Thread URLs (Missing Code)
cat("\nExtracting Premier League threads...\n")
premier_league_threads <- safe_reddit_call(
  find_thread_urls,
  subreddit = "PremierLeague",
  sort_by = "top",
  period = "month"
)

# 4. Data Verification
if (!is.null(premier_league_threads)) {
  cat("\nReddit Data Overview:\n")
  cat("Premier League subreddits found:", 
      ifelse(is.null(premier_league_subreddits), 0, nrow(premier_league_subreddits)), "\n")
  cat("Premier League threads collected:", nrow(premier_league_threads), "\n")
  
  # Display sample data
  cat("\nSample Threads:\n")
  premier_league_threads %>%
    select(title, url, comments) %>%
    head(5) %>%
    knitr::kable()
} else {
  cat("\nFailed to retrieve thread data. Check API limits and internet connection.")
}
```

```{r}
cat("Reddit Data Overview:\n")
cat("Premier League subreddits found:", nrow(premier_league_subreddits), "\n")
cat("Premier League threads collected:", nrow(premier_league_threads), "\n")
```

```{r}
# Extract comments from selected threads
if (!is.null(premier_league_threads) && nrow(premier_league_threads) > 0) {
  # Select a subset of threads to analyze (top 3 by comment count to avoid rate limiting)
  top_threads <- premier_league_threads %>%
    arrange(desc(comments)) %>%
    head(3)
  
  cat("\nExtracting comments from top", nrow(top_threads), "threads...\n")
  
  # Initialize empty data frame for thread content
  thread_content <- list(comments = data.frame())
  
  # Process each thread URL with rate limiting
  for (i in 1:nrow(top_threads)) {
    url <- top_threads$url[i]
    cat("Processing thread", i, "of", nrow(top_threads), ":", substr(top_threads$title[i], 1, 50), "...\n")
    
    # Use safe_reddit_call to get thread content with error handling
    content <- safe_reddit_call(
      get_thread_content,
      url = url,
      # Longer delay for comment extraction as it's more intensive
      delay = 8
    )
    
    # Combine with previous results if content was retrieved
    if (!is.null(content) && !is.null(content$comments) && nrow(content$comments) > 0) {
      thread_content$comments <- rbind(thread_content$comments, content$comments)
    }
    
    # Add extra delay between thread processing
    Sys.sleep(5)
  }
  
  cat("\nExtracted", nrow(thread_content$comments), "comments in total.\n")
} else {
  cat("\nNo Premier League threads available for comment extraction.\n")
  thread_content <- list(comments = data.frame())
}
```

```{r}
# 3.2: Sentiment Analysis of Reddit Comments
# -----------------------------------------

# Process comment data if available
if (!is.null(thread_content) && !is.null(thread_content$comments) && nrow(thread_content$comments) > 0) {
  cat("\nAnalyzing sentiment in", nrow(thread_content$comments), "Reddit comments...\n")
  
  # Tokenize comments for text analysis
  comment_words <- thread_content$comments %>%
    select(comment, author, upvotes) %>%
    unnest_tokens(word, comment) %>%
    anti_join(stop_words, by = "word") %>%
    filter(!grepl("^[0-9]+$", word)) %>%
    filter(nchar(word) > 2)
  
  # Calculate word frequencies
  word_frequencies <- comment_words %>%
    count(word, sort = TRUE) %>%
    head(50)
  
  # Sentiment analysis
  comment_sentiment <- comment_words %>%
    inner_join(get_sentiments("bing"), by = "word") %>%
    count(sentiment) %>%
    spread(sentiment, n, fill = 0) %>%
    mutate(
      total_sentiment_words = positive + negative,
      sentiment_ratio = positive / total_sentiment_words
    )
  
  # Extract player mentions
  player_names <- player_standard_stats %>% 
    pull(Player) %>% 
    unique()
  
  # Function to find player mentions in a text
  find_player_mentions <- function(text, players) {
    mentioned_players <- c()
    for (player in players) {
      if (grepl(player, text, ignore.case = TRUE)) {
        mentioned_players <- c(mentioned_players, player)
      }
    }
    return(paste(mentioned_players, collapse = "|"))
  }
  
  # Apply to comments (limit to a sample to avoid long processing time)
  comment_sample <- thread_content$comments %>%
    sample_n(min(nrow(thread_content$comments), 500)) %>%
    mutate(
      mentioned_players = sapply(comment, function(c) {
        find_player_mentions(c, player_names)
      }),
      has_player_mention = mentioned_players != ""
    )
  
  # Extract player sentiment
  player_mentions <- comment_sample %>%
    filter(has_player_mention) %>%
    separate_rows(mentioned_players, sep = "\\|") %>%
    filter(mentioned_players != "") %>%
    group_by(mentioned_players) %>%
    summarise(
      mention_count = n(),
      avg_upvotes = mean(upvotes, na.rm = TRUE)
    ) %>%
    arrange(desc(mention_count)) %>%
    rename(Player = mentioned_players)
  
  # Visualizations
  cat("\nCreating visualizations of Reddit data analysis...\n")
  
  # Word cloud of most common terms
  set.seed(123)
  p1 <- wordcloud(words = word_frequencies$word, 
            freq = word_frequencies$n, 
            max.words = 100, 
            colors = brewer.pal(8, "Dark2"),
            random.order = FALSE,
            main = "Most Common Terms in Reddit Discussions")
  
  # Bar chart of most mentioned players
  p2 <- ggplot(head(player_mentions, 15), aes(x = reorder(Player, mention_count), y = mention_count)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    coord_flip() +
    theme_minimal() +
    labs(x = "Player", y = "Number of Mentions", 
         title = "Most Discussed Players on Reddit")
}
```

```{r}

# 3.3: Integrate Reddit Data with Player Statistics
# -----------------------------------------------

# Join player mentions with performance data
if (exists("player_mentions") && nrow(player_mentions) > 0) {
  player_integrated <- player_standard_stats %>%
    left_join(player_mentions, by = "Player") %>%
    mutate(
      mention_count = replace_na(mention_count, 0),
      avg_upvotes = replace_na(avg_upvotes, 0)
    )
  
  # Create a popularity vs performance metric
  player_popularity <- player_integrated %>%
    group_by(Player) %>%
    summarize(
      total_mentions = sum(mention_count),
      avg_upvotes = mean(avg_upvotes, na.rm = TRUE),
      total_goals = sum(Gls, na.rm = TRUE),
      total_assists = sum(Ast, na.rm = TRUE),
      performance_score = total_goals + total_assists,
      popularity_score = total_mentions * (avg_upvotes + 1)
    ) %>%
    filter(total_mentions > 0) %>%
    arrange(desc(popularity_score))
  
  # Visualization of popularity vs performance
  p3 <- ggplot(head(player_popularity, 25), 
               aes(x = performance_score, y = popularity_score, label = Player)) +
    geom_point(aes(size = total_mentions), alpha = 0.7) +
    geom_text_repel(size = 3) +
    theme_minimal() +
    labs(x = "Performance Score (Goals + Assists)", 
         y = "Popularity Score (Mentions × Upvotes)",
         title = "Player Performance vs. Reddit Popularity")
}

# 3.4: Prepare Data for Predictive Modeling
# --------------------------------------

# Create a comprehensive dataset for modeling
modeling_data <- player_standard_stats %>%
  left_join(
    player_shooting_stats %>% 
      select(Player, Squad, Season_End_Year, contains("Sh"), contains("Gls"), contains("xG")),
    by = c("Player", "Squad", "Season_End_Year")
  ) %>%
  left_join(
    player_passing_stats %>% 
      select(Player, Squad, Season_End_Year, contains("Pass"), contains("Ast"), contains("xA")),
    by = c("Player", "Squad", "Season_End_Year")
  ) %>%
  left_join(
    player_defensive_stats %>% 
      select(Player, Squad, Season_End_Year, contains("Tkl"), contains("Int"), contains("Blocks")),
    by = c("Player", "Squad", "Season_End_Year")
  )

# Add Reddit popularity metrics if available
if (exists("player_popularity")) {
  modeling_data <- modeling_data %>%
    left_join(
      player_popularity %>% select(Player, popularity_score, total_mentions),
      by = "Player"
    ) %>%
    mutate(
      popularity_score = replace_na(popularity_score, 0),
      total_mentions = replace_na(total_mentions, 0)
    )
}

# Feature engineering
modeling_data <- modeling_data %>%
  mutate(
    # Create position categories
    Position_Group = case_when(
      grepl("FW", Pos) ~ "Forward",
      grepl("MF", Pos) ~ "Midfielder",
      grepl("DF", Pos) ~ "Defender",
      grepl("GK", Pos) ~ "Goalkeeper",
      TRUE ~ "Other"
    ),
    
    # Calculate minutes per match
    Mins_Per_Match = if("Min" %in% names(.)) Min / MP else 0,
    
    # Offensive efficiency metrics
    Goal_Conversion = if("Sh" %in% names(.)) Gls / Sh else NA,
    Shot_Accuracy = if("SoT" %in% names(.)) SoT / Sh else NA,
    
    # Create target variable for high performer classification
    # Different thresholds by position
    High_Performer = case_when(
      Position_Group == "Forward" & Gls >= 10 ~ TRUE,
      Position_Group == "Midfielder" & (Gls + Ast) >= 10 ~ TRUE,
      Position_Group == "Defender" & (Gls + Ast) >= 5 ~ TRUE,
      TRUE ~ FALSE
    )
  ) %>%
  filter(!is.na(Position_Group)) %>% 
  filter(Position_Group != "Goalkeeper") # Exclude goalkeepers from outfield analysis

# Check for missing values
missing_values <- colSums(is.na(modeling_data)) / nrow(modeling_data)
high_missing <- names(missing_values[missing_values > 0.3])

# Remove columns with too many missing values
modeling_data <- modeling_data %>%
  select(-all_of(high_missing))

# Split data by seasons
train_data <- modeling_data %>% filter(Season_End_Year < 2023)
test_data <- modeling_data %>% filter(Season_End_Year == 2023)

# 3.5: Build Predictive Models
# -------------------------

# Set up recipe for preprocessing
model_recipe <- recipe(High_Performer ~ ., data = train_data) %>%
  # Remove identification variables
  step_rm(Player, Squad, URL, Comp) %>%
  # Convert categorical variables to factors
  step_string2factor(Position_Group, Pos) %>%
  # Handle missing values
  step_impute_knn(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  # Remove near-zero variance predictors
  step_nzv(all_predictors()) %>%
  # Normalize numeric variables
  step_normalize(all_numeric_predictors()) %>%
  # Create dummy variables for categorical predictors
  step_dummy(all_nominal_predictors()) %>%
  # Handle class imbalance
  step_smote(High_Performer)

# Random Forest model
rf_spec <- rand_forest(
  trees = 500,
  mtry = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# XGBoost model
xgb_spec <- boost_tree(
  trees = 500,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune(),
  learn_rate = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# Set up workflows
rf_workflow <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(rf_spec)

xgb_workflow <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(xgb_spec)

# Set up cross-validation
set.seed(123)
cv_folds <- vfold_cv(train_data, v = 5, strata = High_Performer)

# Tune hyperparameters (minimal for demonstration)
rf_grid <- grid_regular(
  mtry(range = c(5, 15)),
  min_n(range = c(2, 8)),
  levels = 3
)

# Tune Random Forest model
cat("\nTuning Random Forest model...\n")
rf_tune_results <- rf_workflow %>%
  tune_grid(
    resamples = cv_folds,
    grid = rf_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall)
  )

# Get best Random Forest model
best_rf <- rf_tune_results %>% 
  select_best(metric = "roc_auc")

# Finalize workflow
final_rf_workflow <- rf_workflow %>%
  finalize_workflow(best_rf)

# Fit final model
cat("Fitting final Random Forest model...\n")
rf_fit <- final_rf_workflow %>%
  fit(train_data)

# 3.6: Model Evaluation and Interpretation
# -------------------------------------

# Evaluate on test data
rf_preds <- predict(rf_fit, test_data) %>%
  bind_cols(predict(rf_fit, test_data, type = "prob")) %>%
  bind_cols(test_data %>% select(Player, Position_Group, High_Performer))

# Calculate metrics
rf_metrics <- rf_preds %>%
  metrics(truth = High_Performer, estimate = .pred_class,
          .pred_TRUE)

cat("\nRandom Forest Model Performance:\n")
print(rf_metrics)

# Extract feature importance
importance_data <- rf_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 15)

# Plot feature importance
p4 <- importance_data %>%
  ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "darkblue") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Features", y = "Importance", 
       title = "Top 15 Features for Predicting High-Performing Players")

# 3.7: Player Recommendations and Predictions
# ----------------------------------------

# Identify undervalued players
undervalued_players <- rf_preds %>%
  filter(.pred_TRUE > 0.7 & High_Performer == FALSE) %>%
  arrange(desc(.pred_TRUE)) %>%
  left_join(
    modeling_data %>% select(Player, Squad, Position_Group, Gls, Ast, Age),
    by = c("Player", "Position_Group")
  ) %>%
  distinct(Player, .keep_all = TRUE) %>%
  select(Player, Squad, Position_Group, Gls, Ast, Age, .pred_TRUE) %>%
  rename(
    Prediction_Score = .pred_TRUE
  )

# Identify young talents
young_talents <- modeling_data %>%
  filter(Age <= 23) %>%
  select(Player, Squad, Age, Position_Group) %>%
  distinct(Player, .keep_all = TRUE) %>%
  inner_join(
    rf_preds %>% 
      select(Player, .pred_TRUE) %>%
      filter(.pred_TRUE > 0.6),
    by = "Player"
  ) %>%
  arrange(desc(.pred_TRUE)) %>%
  rename(
    Prediction_Score = .pred_TRUE
  )

# Display recommendations
cat("\nPotentially Undervalued Players (High prediction score but not yet high performers):\n")
knitr::kable(head(undervalued_players, 10), format = "simple")

cat("\nPromising Young Talents (23 or younger with high potential):\n")
knitr::kable(head(young_talents, 10), format = "simple")

# 3.8: Position-Specific Analysis
# ----------------------------

# Analyze different metrics by position
position_analysis <- modeling_data %>%
  group_by(Position_Group) %>%
  summarize(
    Player_Count = n_distinct(Player),
    Avg_Age = mean(Age, na.rm = TRUE),
    Avg_Goals = mean(Gls, na.rm = TRUE),
    Avg_Assists = mean(Ast, na.rm = TRUE),
    Shots_Per_Goal = mean(Sh / Gls, na.rm = TRUE),
    High_Performer_Rate = mean(High_Performer, na.rm = TRUE)
  )

# Display position analysis
cat("\nPosition-Specific Performance Analysis:\n")
knitr::kable(position_analysis, format = "simple", digits = 2)

# Position-specific visualizations
p5 <- ggplot(modeling_data, aes(x = Age, y = Gls, color = Position_Group)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", se = FALSE) +
  facet_wrap(~ Position_Group) +
  theme_minimal() +
  labs(x = "Age", y = "Goals Scored", 
       title = "Goals Scored by Age Across Different Positions")

# Save visualizations and models
cat("\nSaving visualization and model objects...\n")
model_outputs <- list(
  rf_model = rf_fit,
  model_metrics = rf_metrics,
  feature_importance = importance_data,
  undervalued_players = undervalued_players,
  young_talents = young_talents
)

save(model_outputs, file = "premier_league_model_results.RData")
cat("Analysis complete! Model results saved to 'premier_league_model_results.RData'")
```




```{r}
# 4: Machine Learning for Transfer Market Analysis
# ===============================================

library(tidymodels)
library(vip)
library(ranger)
library(ggplot2)
library(corrr)
library(plotly)
library(forcats)

# 4.1: Data Preparation for Transfer Analysis
# -----------------------------------------

# First, let's add estimated market value as a target variable
# We'll simulate it based on player performance metrics
# In a real analysis, you would use actual transfer fee data

set.seed(123) # For reproducibility

# Create a comprehensive player dataset
player_analysis <- player_standard_stats %>%
  # Join with shooting stats
  left_join(
    player_shooting_stats %>%
      select(Player, Squad, Season_End_Year, Sh, SoT, Sh_per_90, contains("xG")),
    by = c("Player", "Squad", "Season_End_Year")
  ) %>%
  # Join with passing stats
  left_join(
    player_passing_stats %>%
      select(Player, Squad, Season_End_Year, Cmp_percent, KP, contains("xA")),
    by = c("Player", "Squad", "Season_End_Year")
  ) %>%
  # Join with defensive stats
  left_join(
    player_defensive_stats %>%
      select(Player, Squad, Season_End_Year, Tkl, Int, Clr),
    by = c("Player", "Squad", "Season_End_Year")
  ) %>%
  # Join with possession stats
  left_join(
    player_possession_stats %>%
      select(Player, Squad, Season_End_Year, contains("Touches"), contains("Carries"), contains("Prog")),
    by = c("Player", "Squad", "Season_End_Year")
  )

# Add position grouping and minutes calculation
player_analysis <- player_analysis %>%
  mutate(
    # Position grouping
    Position_Group = case_when(
      grepl("FW", Pos) ~ "Forward",
      grepl("MF", Pos) ~ "Midfielder",
      grepl("DF", Pos) ~ "Defender",
      grepl("GK", Pos) ~ "Goalkeeper",
      TRUE ~ "Other"
    ),
    
    # Minutes calculation
    Minutes = 90 * MP_Playing,
    
    # Age group for analyzing value trends
    Age_Group = case_when(
      Age < 21 ~ "Under 21",
      Age >= 21 & Age <= 23 ~ "21-23",
      Age >= 24 & Age <= 27 ~ "24-27",
      Age >= 28 & Age <= 30 ~ "28-30",
      Age > 30 ~ "Over 30"
    ),
    
    # Create synthetic market value based on player metrics
    # This formula simulates how the market might value different attributes
    Market_Value_Million = case_when(
      Position_Group == "Forward" ~ (0.8 * Gls + 0.3 * Ast + 0.1 * Age - 0.15 * pmax(Age - 28, 0)^2) * (1 + 0.2 * (MP_Playing/38)),
      Position_Group == "Midfielder" ~ (0.4 * Gls + 0.6 * Ast + 0.1 * KP + 0.1 * Age - 0.15 * pmax(Age - 28, 0)^2) * (1 + 0.2 * (MP_Playing/38)),
      Position_Group == "Defender" ~ (0.2 * (Tkl + Int + Clr)/10 + 0.3 * Ast + 0.4 * Gls + 0.1 * Age - 0.15 * pmax(Age - 28, 0)^2) * (1 + 0.2 * (MP_Playing/38)),
      Position_Group == "Goalkeeper" ~ (0.2 * (Saves/20) + 0.1 * Age - 0.15 * pmax(Age - 28, 0)^2) * (1 + 0.2 * (MP_Playing/38)),
      TRUE ~ 1.0
    ),
    
    # Add some realistic variance
    Market_Value_Million = pmax(0.5, Market_Value_Million * (1 + rnorm(n(), mean = 0, sd = 0.2)))
  )

# Filter out players with very limited playing time
player_analysis <- player_analysis %>%
  filter(Minutes >= 450) # At least 5 games

# 4.2: Exploratory Visualizations for Transfer Analysis
# --------------------------------------------------

# 1. Market Value Distribution by Position
market_by_position <- player_analysis %>%
  group_by(Position_Group) %>%
  summarize(
    Avg_Value = mean(Market_Value_Million, na.rm = TRUE),
    Med_Value = median(Market_Value_Million, na.rm = TRUE),
    Max_Value = max(Market_Value_Million, na.rm = TRUE),
    Min_Value = min(Market_Value_Million, na.rm = TRUE),
    StdDev = sd(Market_Value_Million, na.rm = TRUE),
    Count = n()
  ) %>%
  arrange(desc(Avg_Value))

# Plot position value comparison
p1 <- ggplot(player_analysis, aes(x = Position_Group, y = Market_Value_Million, fill = Position_Group)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  theme_minimal() +
  labs(title = "Market Value Distribution by Position",
       x = "Position",
       y = "Estimated Value (€ Million)",
       fill = "Position") +
  theme(legend.position = "none")

print(p1)

# 2. Age-Value Relationship
p2 <- ggplot(player_analysis, aes(x = Age, y = Market_Value_Million, color = Position_Group)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", se = FALSE) +
  facet_wrap(~ Position_Group) +
  theme_minimal() +
  labs(title = "Age vs. Market Value by Position",
       x = "Age",
       y = "Estimated Value (€ Million)",
       color = "Position")

print(p2)

# 3. Top Valued Players
top_players <- player_analysis %>%
  group_by(Player) %>%
  filter(Season_End_Year == max(Season_End_Year)) %>% # Most recent season
  ungroup() %>%
  arrange(desc(Market_Value_Million)) %>%
  head(15)

p3 <- ggplot(top_players, aes(x = reorder(Player, Market_Value_Million), y = Market_Value_Million)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top 15 Most Valuable Players",
       x = "Player",
       y = "Estimated Value (€ Million)")

print(p3)

# 4. Value vs. Goals+Assists
p4 <- ggplot(player_analysis, aes(x = Gls + Ast, y = Market_Value_Million, color = Position_Group)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(title = "Goal Contributions vs. Market Value",
       x = "Goals + Assists",
       y = "Estimated Value (€ Million)",
       color = "Position")

print(p4)

# 4.3: Machine Learning Model for Value Prediction
# ----------------------------------------------

# Split data into training and testing
set.seed(456)
player_split <- initial_split(player_analysis, prop = 0.75)
player_train <- training(player_split)
player_test <- testing(player_split)

# Create recipe for preprocessing
value_recipe <- recipe(Market_Value_Million ~ ., data = player_train) %>%
  # Remove identification variables and URL
  step_rm(Player, Squad, URL) %>%
  # Convert categorical variables to factors
  step_string2factor(Position_Group, Age_Group, Pos, Comp) %>%
  # Handle missing values
  step_impute_knn(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  # Remove near-zero variance predictors
  step_nzv(all_predictors()) %>%
  # Normalize numeric variables
  step_normalize(all_numeric_predictors()) %>%
  # Create dummy variables for categorical predictors
  step_dummy(all_nominal_predictors())

# Random Forest model specification
rf_spec <- rand_forest(
  trees = 500,
  mtry = floor(sqrt(ncol(player_train))),
  min_n = 5
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# Create workflow
rf_workflow <- workflow() %>%
  add_recipe(value_recipe) %>%
  add_model(rf_spec)

# Fit the model
cat("\nFitting Random Forest model for transfer value prediction...\n")
rf_fit <- rf_workflow %>%
  fit(player_train)

# Make predictions
predictions <- predict(rf_fit, player_test) %>%
  bind_cols(player_test %>% select(Player, Position_Group, Age, Market_Value_Million))

# Calculate metrics
eval_metrics <- metrics(predictions, truth = Market_Value_Million, estimate = .pred)

cat("\nModel Evaluation Metrics:\n")
print(eval_metrics)

# 4.4: Feature Importance
# ---------------------

# Extract feature importance
importance_data <- rf_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 15)

# Plot feature importance
p5 <- importance_data %>%
  ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "darkblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top 15 Features for Predicting Player Market Value",
       x = "Features",
       y = "Importance")

print(p5)

# 4.5: Undervalued & Overvalued Players Analysis
# -------------------------------------------

# Calculate valuation difference
valuation_analysis <- predictions %>%
  mutate(
    Predicted_Value = .pred,
    Actual_Value = Market_Value_Million,
    Value_Difference = Predicted_Value - Actual_Value,
    Value_Difference_Percent = (Value_Difference / Actual_Value) * 100
  )

# Find undervalued players (players worth more than their stats suggest)
undervalued_players <- valuation_analysis %>%
  filter(Value_Difference < -1) %>% # At least €1M undervalued
  arrange(Value_Difference) %>%
  head(10)

# Find overvalued players (players worth less than their stats suggest)
overvalued_players <- valuation_analysis %>%
  filter(Value_Difference > 1) %>% # At least €1M overvalued
  arrange(desc(Value_Difference)) %>%
  head(10)

cat("\nPotentially Undervalued Players (Market values them less than model):\n")
knitr::kable(
  undervalued_players %>% 
    select(Player, Position_Group, Age, Actual_Value, Predicted_Value, Value_Difference),
  digits = 2,
  format = "simple"
)

cat("\nPotentially Overvalued Players (Market values them more than model):\n")
knitr::kable(
```
```{r}
```

```{r}
```

```{r}
```

```{r}
```

```{r}
```


```{r}
```

```{r}
```

```{r}
```


```{r}
```


```{r}
```

```{r}
```

```{r}
```


```{r}
```

```{r}
```

```{r}
```


```{r}
```


```{r}
```

```{r}
```

```{r}
```

```{r}
```

```{r}
library(rvest)
library(dplyr)
library(stringr)
library(tidyr)

# Function to scrape match data from Premier League website
scrape_pl_matches <- function(season) {
  base_url <- paste0("https://www.premierleague.com/results?co=1&se=", season)
  
  webpage <- read_html(base_url)
  
  # Extract match information
  matches <- webpage %>%
    html_nodes(".matchFixtureContainer") %>%
    html_text() %>%
    str_trim()
  
  # Process and structure the data
  match_data <- tibble(match_info = matches) %>%
    # Additional processing to extract teams, scores, etc.
    # This would require specific parsing based on the HTML structure
    
  return(match_data)
}

# Scrape transfer data
scrape_transfers <- function(season) {
  base_url <- paste0("https://www.premierleague.com/transfers/summer/", season)
  
  webpage <- read_html(base_url)
  
  # Extract transfer information
  # This would require specific parsing based on the HTML structure
  
  return(transfer_data)
}
```

```{r}
library(tidyverse)
library(lubridate)

# Combine player statistics from different sources
prepare_player_data <- function(standard_stats, shooting_stats, passing_stats, defensive_stats) {
  # Join datasets on player ID or name
  combined_stats <- standard_stats %>%
    left_join(shooting_stats, by = c("Player", "Team")) %>%
    left_join(passing_stats, by = c("Player", "Team")) %>%
    left_join(defensive_stats, by = c("Player", "Team"))
  
  # Clean data: handle missing values
  combined_stats <- combined_stats %>%
    mutate_all(~ifelse(is.na(.), 0, .))
  
  # Feature engineering
  processed_data <- combined_stats %>%
    mutate(
      goals_per_90 = Goals / (Minutes / 90),
      assists_per_90 = Assists / (Minutes / 90),
      goal_creating_actions_per_90 = GCA / (Minutes / 90),
      # Create performance index
      performance_index = (Goals * 3 + Assists * 2 + GCA * 1) / (Minutes / 90)
    ) %>%
    # Filter for players with significant minutes
    filter(Minutes >= 450)  # At least 5 full matches
  
  return(processed_data)
}

# Process and aggregate team formation data
process_formations <- function(match_data) {
  formation_stats <- match_data %>%
    group_by(Team, Formation) %>%
    summarize(
      matches_played = n(),
      wins = sum(Result == "W"),
      draws = sum(Result == "D"),
      losses = sum(Result == "L"),
      goals_scored = sum(GoalsFor),
      goals_conceded = sum(GoalsAgainst),
      win_percentage = wins / matches_played * 100
    ) %>%
    ungroup()
  
  return(formation_stats)
}
```

```{r}
library(ggplot2)
library(ggsoccer)  # For soccer pitch visualizations
library(plotly)

# Visualize player positions and their performance metrics
visualize_positions <- function(player_data) {
  # Create a pitch visualization with player positions
  pitch_plot <- ggplot() +
    annotate_pitch() +
    theme_pitch() +
    geom_point(data = player_data, 
               aes(x = position_x, y = position_y, color = performance_index, 
                   size = minutes_played, text = player_name),
               alpha = 0.7) +
    scale_color_gradient(low = "blue", high = "red") +
    labs(title = "Player Positions and Performance",
         subtitle = "Size indicates minutes played, color indicates performance index",
         color = "Performance Index",
         size = "Minutes Played") +
    theme(legend.position = "bottom")
  
  # Convert to interactive plot
  interactive_plot <- ggplotly(pitch_plot, tooltip = "text")
  
  return(interactive_plot)
}

# Visualize team formations
visualize_formations <- function(formation_data) {
  formation_plot <- formation_data %>%
    group_by(Formation) %>%
    summarize(
      total_matches = sum(matches_played),
      average_win_percentage = weighted.mean(win_percentage, matches_played)
    ) %>%
    ggplot(aes(x = reorder(Formation, average_win_percentage), 
               y = average_win_percentage, 
               fill = total_matches)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    scale_fill_gradient(low = "lightblue", high = "darkblue") +
    labs(title = "Effectiveness of Different Formations",
         subtitle = "Average win percentage by formation",
         x = "Formation",
         y = "Win Percentage (%)",
         fill = "Total Matches") +
    theme_minimal()
  
  return(formation_plot)
}
```

```{r}
# Visualize transfer market trends
visualize_transfers <- function(transfer_data) {
  # Transfer fees by position
  position_plot <- transfer_data %>%
    group_by(Position) %>%
    summarize(
      avg_fee = mean(Fee, na.rm = TRUE),
      total_fee = sum(Fee, na.rm = TRUE),
      count = n()
    ) %>%
    ggplot(aes(x = reorder(Position, avg_fee), y = avg_fee, size = count, color = total_fee)) +
    geom_point() +
    scale_color_gradient(low = "green", high = "red") +
    coord_flip() +
    labs(title = "Transfer Market Analysis by Position",
         subtitle = "Average transfer fee by position",
         x = "Position",
         y = "Average Transfer Fee (£ millions)",
         size = "Number of Transfers",
         color = "Total Spent (£ millions)") +
    theme_minimal()
  
  # Transfer network between teams
  network_plot <- ggplot(transfer_data, aes(x = Selling_Club, y = Buying_Club)) +
    geom_point(aes(size = Fee, color = Fee)) +
    scale_size_continuous(range = c(1, 10)) +
    scale_color_gradient(low = "blue", high = "red") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    labs(title = "Transfer Network Between Clubs",
         x = "Selling Club",
         y = "Buying Club",
         size = "Transfer Fee",
         color = "Transfer Fee (£ millions)")
  
  return(list(position_plot = position_plot, network_plot = network_plot))
}
```

```{r}
library(tidymodels)
library(caret)
library(ranger)  # For random forests
library(glmnet)  # For ridge/lasso regression

# Split data into training and testing sets
prepare_ml_data <- function(player_data, target_variable = "performance_index") {
  # Create the target variable if it's a formula
  if (!target_variable %in% colnames(player_data)) {
    # This is just an example, modify based on your specific needs
    player_data <- player_data %>%
      mutate(performance_index = (Goals * 3 + Assists * 2 + Shots / 2) / (Minutes / 90))
  }
  
  # Select relevant features
  model_data <- player_data %>%
    select(Minutes, Goals, Assists, Shots, SoT, Passes, KeyPasses, Tackles, 
           Interceptions, Position, Age, performance_index)
  
  # Split data
  set.seed(123)
  split <- initial_split(model_data, prop = 0.75)
  train_data <- training(split)
  test_data <- testing(split)
  
  return(list(train = train_data, test = test_data, full = model_data))
}

# Train multiple models and compare performance
train_models <- function(train_data, test_data, target = "performance_index") {
  # Create formula
  formula_str <- paste(target, "~ .")
  formula_obj <- as.formula(formula_str)
  
  # Set up cross-validation
  train_control <- trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
  
  # 1. Linear Regression
  linear_model <- train(
    formula_obj,
    data = train_data,
    method = "lm",
    trControl = train_control
  )
  
  # 2. Ridge Regression
  ridge_model <- train(
    formula_obj,
    data = train_data,
    method = "glmnet",
    tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 1, length = 20)),
    trControl = train_control
  )
  
  # 3. Lasso Regression
  lasso_model <- train(
    formula_obj,
    data = train_data,
    method = "glmnet",
    tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 20)),
    trControl = train_control
  )
  
  # 4. Random Forest
  rf_model <- train(
    formula_obj,
    data = train_data,
    method = "ranger",
    trControl = train_control,
    tuneLength = 5,
    importance = 'impurity'
  )
  
  # 5. Gradient Boosting
  gbm_model <- train(
    formula_obj,
    data = train_data,
    method = "gbm",
    trControl = train_control,
    verbose = FALSE
  )
  
  # Make predictions on test data
  predict_and_evaluate <- function(model, name) {
    preds <- predict(model, newdata = test_data)
    rmse <- sqrt(mean((preds - test_data[[target]])^2))
    r2 <- cor(preds, test_data[[target]])^2
    return(data.frame(Model = name, RMSE = rmse, R_squared = r2))
  }
  
  # Evaluate all models
  eval_results <- rbind(
    predict_and_evaluate(linear_model, "Linear Regression"),
    predict_and_evaluate(ridge_model, "Ridge Regression"),
    predict_and_evaluate(lasso_model, "Lasso Regression"),
    predict_and_evaluate(rf_model, "Random Forest"),
    predict_and_evaluate(gbm_model, "Gradient Boosting")
  )
  
  models_list <- list(
    linear = linear_model,
    ridge = ridge_model,
    lasso = lasso_model,
    rf = rf_model,
    gbm = gbm_model
  )
  
  return(list(models = models_list, evaluation = eval_results))
}

```

```{r}
# Create an xG model using R and tidymodels
build_xg_model <- function(shots_data) {
  library(tidymodels)
  
  # Prepare data
  shots_data <- shots_data %>%
    mutate(
      # Calculate distance from goal
      distance = sqrt((location_x - 100)^2 + (location_y - 50)^2),
      # Calculate angle to goal (simplified version)
      angle = abs(atan2(location_y - 50, 100 - location_x))
    )
  
  # Split the data
  set.seed(123)
  shots_split <- initial_split(shots_data, prop = 0.75, strata = is_goal)
  shots_train <- training(shots_split)
  shots_test <- testing(shots_split)
  
  # Create recipe for preprocessing
  shots_recipe <- recipe(is_goal ~ distance + angle + header + big_chance + 
                           from_corner + from_fk + direct_fk, 
                         data = shots_train) %>%
    step_normalize(all_predictors())
  
  # Create workflow with logistic regression
  xg_workflow <- workflow() %>%
    add_recipe(shots_recipe) %>%
    add_model(logistic_reg())
  
  # Fit the model
  xg_fit <- fit(xg_workflow, data = shots_train)
  
  # Evaluate model
  shots_test_pred <- predict(xg_fit, shots_test, type = "prob") %>%
    bind_cols(shots_test)
  
  # Calculate performance metrics
  roc_auc <- roc_auc(shots_test_pred, truth = is_goal, .pred_1)
  
  # Create visual representation of xG
  create_xg_heatmap <- function(model) {
    # Create a grid of locations on the pitch
    pitch_grid <- expand.grid(
      location_x = seq(50, 100, by = 0.5),
      location_y = seq(0, 100, by = 0.5)
    ) %>%
      as_tibble() %>%
      mutate(
        distance = sqrt((location_x - 100)^2 + (location_y - 50)^2),
        angle = abs(atan2(location_y - 50, 100 - location_x)),
        header = 0,
        big_chance = 0,
        from_corner = 0,
        from_fk = 0,
        direct_fk = 0
      )
    
    # Predict xG for each location
    pitch_grid_pred <- predict(model, pitch_grid, type = "prob") %>%
      bind_cols(pitch_grid) %>%
      rename(xG = .pred_1)
    
    # Create heatmap
    heatmap <- ggplot(pitch_grid_pred, aes(x = location_x, y = location_y, fill = xG)) +
      geom_tile() +
      scale_fill_gradient(low = "blue", high = "red") +
      coord_fixed(ratio = 68/105) +
      labs(title = "Expected Goals (xG) Model",
           subtitle = "xG probability by shot location",
           fill = "xG Probability") +
      theme_minimal()
    
    return(heatmap)
  }
  
  heatmap <- create_xg_heatmap(xg_fit)
  
  return(list(model = xg_fit, roc_auc = roc_auc, heatmap = heatmap))
}
```




```{r}
# Premier League Soccer Analytics
# ==============================
# This script collects, analyzes, and visualizes Premier League data
# to provide insights into player performance, team tactics, and transfer market dynamics

# Load required libraries
library(worldfootballR)  # For football data collection
library(tidyverse)       # For data manipulation and visualization
library(tidymodels)      # For machine learning analysis
library(vip)             # For visualizing variable importance
library(ggsoccer)        # For soccer pitch visualizations
library(RColorBrewer)    # For color palettes
library(plotly)          # For interactive plots
library(ggrepel)         # For text labels that don't overlap
library(ranger)          # For random forest models
library(patchwork)       # For combining multiple plots

# Set random seed for reproducibility
set.seed(123)
```

```{r}
# 1. Data Collection
# =================

# 1.1 Collect Premier League data from fbref.com using worldfootballR
# Getting data from the 2019-2020, 2020-2021, 2021-2022, and 2022-2023 seasons
player_seasons <- c("2019-2020", "2020-2021", "2021-2022", "2022-2023")

# Initialize empty data frames for each type of statistics
player_standard_stats <- data.frame()
player_shooting_stats <- data.frame()
player_passing_stats <- data.frame()
player_defensive_stats <- data.frame()
player_possession_stats <- data.frame()
player_gk_stats <- data.frame()

# Function to safely collect data with error handling
collect_player_data <- function(season_end_year, stat_type) {
  cat(paste0("Collecting ", stat_type, " statistics for season ending in ", season_end_year, "...\n"))
  
  tryCatch({
    # Use worldfootballR to get the data
    temp_data <- fb_big5_advanced_season_stats(
      season_end_year = season_end_year,
      stat_type = stat_type,
      team_or_player = "player"
    )
    
    # Filter for Premier League only
    temp_data <- temp_data %>% 
      filter(grepl("Premier League", Comp))
    
    # Add a short delay to avoid overwhelming the server
    Sys.sleep(2)
    
    return(temp_data)
  }, 
  error = function(e) {
    cat("Error collecting data:", e$message, "\n")
    return(data.frame())
  })
}

# Looping through seasons to collect data
for (season in player_seasons) {
  cat("\n==== Collecting data for season:", season, "====\n")
  season_end_year <- as.numeric(substr(season, 6, 9))
  
  # Collect standard statistics
  temp_standard <- collect_player_data(season_end_year, "standard")
  if (nrow(temp_standard) > 0) {
    player_standard_stats <- bind_rows(player_standard_stats, temp_standard)
  }
  
  # Collect shooting statistics
  temp_shooting <- collect_player_data(season_end_year, "shooting")
  if (nrow(temp_shooting) > 0) {
    player_shooting_stats <- bind_rows(player_shooting_stats, temp_shooting)
  }
  
  # Collect passing statistics
  temp_passing <- collect_player_data(season_end_year, "passing")
  if (nrow(temp_passing) > 0) {
    player_passing_stats <- bind_rows(player_passing_stats, temp_passing)
  }
  
  # Collect defensive statistics
  temp_defensive <- collect_player_data(season_end_year, "defense")
  if (nrow(temp_defensive) > 0) {
    player_defensive_stats <- bind_rows(player_defensive_stats, temp_defensive)
  }
  
  # Collect possession statistics
  temp_possession <- collect_player_data(season_end_year, "possession")
  if (nrow(temp_possession) > 0) {
    player_possession_stats <- bind_rows(player_possession_stats, temp_possession)
  }
  
  # Collect goalkeeper statistics
  temp_gk <- collect_player_data(season_end_year, "keeper")
  if (nrow(temp_gk) > 0) {
    player_gk_stats <- bind_rows(player_gk_stats, temp_gk)
  }
  
  Sys.sleep(5)  # Longer delay between seasons
  cat("Completed collecting data for", season, "\n")
}

# Display summary of collected data
cat("\n==== Data Collection Summary ====\n")
cat("Standard stats:", nrow(player_standard_stats), "records\n")
cat("Shooting stats:", nrow(player_shooting_stats), "records\n")
cat("Passing stats:", nrow(player_passing_stats), "records\n")
cat("Defensive stats:", nrow(player_defensive_stats), "records\n")
cat("Possession stats:", nrow(player_possession_stats), "records\n")
cat("Goalkeeper stats:", nrow(player_gk_stats), "records\n")

# Save raw data to avoid repeated API calls
save(player_standard_stats, player_shooting_stats, player_passing_stats, 
     player_defensive_stats, player_possession_stats, player_gk_stats,
     file = "premier_league_raw_data.RData")
```

```{r}
# 2. Data Processing and Integration
# =================================

# 2.1 Examine and standardize data structures
# Let's look at column names across datasets to ensure consistency
identify_key_columns <- function() {
  # Identify key joining columns
  standard_cols <- colnames(player_standard_stats)[1:10]
  
  cat("Standard stats key columns:\n")
  print(standard_cols)
  
  # Check for minutes column naming
  minutes_cols <- grep("Min|MP", colnames(player_standard_stats), value = TRUE)
  cat("\nPotential minutes columns:\n")
  print(minutes_cols)
}

identify_key_columns()
```

```{r}
# 2.2 Create a unified player performance dataset
create_unified_dataset <- function() {
  # First standardize minutes column
  player_standard_stats <- player_standard_stats %>%
    mutate(
      # Attempt to standardize minutes played
      Minutes_Played = case_when(
        "Min" %in% names(.) ~ Min,
        "MP_Playing" %in% names(.) ~ MP_Playing * 90,
        "MP" %in% names(.) ~ MP * 90,
        TRUE ~ 0
      )
    )
  
  # Join all datasets
  unified_data <- player_standard_stats %>%
    # Join with shooting stats
    left_join(
      player_shooting_stats %>%
        select(Player, Squad, Season_End_Year, contains("Sh"), contains("Gls"), contains("xG")),
      by = c("Player", "Squad", "Season_End_Year")
    ) %>%
    # Join with passing stats
    left_join(
      player_passing_stats %>%
        select(Player, Squad, Season_End_Year, contains("Cmp"), contains("Att"), contains("xA"), KP),
      by = c("Player", "Squad", "Season_End_Year")
    ) %>%
    # Join with defensive stats
    left_join(
      player_defensive_stats %>%
        select(Player, Squad, Season_End_Year, Tkl, Int, Blocks, Clr),
      by = c("Player", "Squad", "Season_End_Year")
    ) %>%
    # Join with possession stats
    left_join(
      player_possession_stats %>%
        select(Player, Squad, Season_End_Year, contains("Touches"), contains("Carries"), contains("Succ")),
      by = c("Player", "Squad", "Season_End_Year")
    )
  
  # Add goalkeeper stats separately (as they have different metrics)
  gk_data <- player_gk_stats %>%
    select(Player, Squad, Season_End_Year, contains("Save"), contains("GA"), contains("PSxG"))
  
  unified_data <- unified_data %>%
    left_join(gk_data, by = c("Player", "Squad", "Season_End_Year"))
  
  return(unified_data)
}

# Create the unified dataset
player_data <- create_unified_dataset()

# 2.3 Feature engineering for player performance analysis
player_data <- player_data %>%
  mutate(
    # Create position groupings
    Position_Group = case_when(
      grepl("FW", Pos) ~ "Forward",
      grepl("MF", Pos) ~ "Midfielder",
      grepl("DF", Pos) ~ "Defender",
      grepl("GK", Pos) ~ "Goalkeeper",
      TRUE ~ "Other"
    ),
    
    # Create per-90 metrics
    Minutes_90 = Minutes_Played / 90,
    
    # Offensive metrics per 90
    Goals_90 = ifelse(Minutes_90 > 0, Gls / Minutes_90, 0),
    Assists_90 = ifelse(Minutes_90 > 0, Ast / Minutes_90, 0),
    Goal_Contributions_90 = Goals_90 + Assists_90,
    
    # Shooting efficiency
    Shot_Conversion = ifelse(Sh > 0, Gls / Sh * 100, 0),
    Shots_On_Target_Pct = ifelse(Sh > 0, SoT / Sh * 100, 0),
    
    # Expected goals differential
    xG_Differential = ifelse(!is.na(xG), Gls - xG, NA),
    
    # Passing efficiency
    Pass_Completion = ifelse(!is.na(Cmp_percent), Cmp_percent, NA),
    
    # Defensive metrics per 90
    Tackles_90 = ifelse(Minutes_90 > 0, Tkl / Minutes_90, 0),
    Interceptions_90 = ifelse(Minutes_90 > 0, Int / Minutes_90, 0),
    
    # Talent development metric (for young players)
    Young_Talent = ifelse(Age <= 23, 
                           (Goal_Contributions_90 * 3 + Minutes_Played/900), 0),
    
    # Experience value (for veterans)
    Veteran_Value = ifelse(Age >= 30, 
                            (Goal_Contributions_90 * 2 + Minutes_Played/1800), 0)
  )

# 2.4 Create synthetic market value (since we can't easily get real transfer values)
player_data <- player_data %>%
  mutate(
    # Base value depends on position and key stats
    Base_Value_Million = case_when(
      Position_Group == "Forward" ~ 2 + (0.8 * Gls + 0.4 * Ast) * (1 + Minutes_Played/3000),
      Position_Group == "Midfielder" ~ 1.5 + (0.5 * Gls + 0.7 * Ast) * (1 + Minutes_Played/3000),
      Position_Group == "Defender" ~ 1 + (1.5 * (Tkl + Int)/30 + 0.3 * Gls + 0.3 * Ast) * (1 + Minutes_Played/3000),
      Position_Group == "Goalkeeper" ~ 0.8 + (Save_percent / 100 * 3) * (1 + Minutes_Played/3000),
      TRUE ~ 0.5
    ),
    
    # Age factor - peak value at age 26-27
    Age_Factor = case_when(
      Age < 23 ~ 1.2 + (23 - Age) * 0.05, # Youth premium
      Age >= 23 & Age <= 27 ~ 1.3,        # Prime years premium
      Age > 27 ~ 1.3 - (Age - 27) * 0.1   # Declining value with age
    ),
    
    # Calculate final market value with some randomness
    Market_Value_Million = pmax(0.5, Base_Value_Million * Age_Factor * 
                                (1 + rnorm(n(), mean = 0, sd = 0.2)))
  )

# Filter to players with reasonable minutes
player_data_filtered <- player_data %>%
  filter(Minutes_Played >= 450) # At least 5 full matches

# Save processed data
save(player_data, player_data_filtered, file = "premier_league_processed_data.RData")
```
