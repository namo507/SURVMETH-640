---
title: "Assignment 1"
output:
  html_document:
    df_print: paged
---

## Setup

```{r results='hide', message=FALSE, warning=FALSE}
library(titanic)
library(caret)
library(pROC)
```

## Data

In this notebook we use the Titanic data that is used on Kaggle (https://www.kaggle.com) as an introductory competition for getting familiar with machine learning. It includes information on a set of Titanic passengers, such as age, sex, ticket class and whether he or she survived the Titanic tragedy.

Source: https://www.kaggle.com/c/titanic/data

```{r}
titanic <- titanic_train
str(titanic)
```

We begin with some minor data preparations. The `lapply()` function is a handy tool if the task is to apply the same transformation (e.g. `as.factor()`) to multiple columns of a data frame.

```{r}
titanic[, c(2:3,5,12)] <- lapply(titanic[, c(2:3,5,12)], as.factor)
```

The `age` variable has some NAs, as a quick and dirty solution we can create a categorized age variable with NAs as an additional factor level.

```{r}
titanic$Age_c <- cut(titanic$Age, 5)
titanic$Age_c <- addNA(titanic$Age_c)
summary(titanic$Age_c)
```

## Train and test set

Next we split the data into a training (80%) and a test (20%) part. This can be done by random sampling with `sample()`.

```{r}
set.seed(9395)
# Add code here
# Calculating the number of rows for training (80%)
n_train <- round(0.8 * nrow(titanic))

# Creating the indices for random sampling
train_indices <- sample(seq_len(nrow(titanic)), size = n_train)

# Creating training and test sets
train_set <- titanic[train_indices, ]
test_set <- titanic[-train_indices, ]

train_set
test_set
```

## Logistic regression

In this exercise we simply use logistic regression as our prediction method, since we want to focus on the evaluation part. Build a first logit model with `Survived` as the outcome and `Pclass`, `Sex`, `Age_c`, `Fare` and `Embarked` as features.

```{r}
# Fitting logistic regression model
model1 <- glm(Survived ~ Pclass + Sex + Age_c + Fare + Embarked,
              data = train_set,
              family = "binomial")

# View model summary
summary(model1)
```

A quick look at the coefficients of the first logit model.

```{r}
coef(model1)
```

Now, build an additional logit model that uses the same features, but includes at least one interaction or non-linear term.

```{r}
# Creating a second model with interaction
model2 <- glm(Survived ~ Pclass + Sex + Age_c + Fare + Embarked + Sex:Pclass,
              data = train_set, 
              family = "binomial")
```

Again, summarize the resulting object.

```{r}
summary(model2)
```

## Prediction in test set

Given both logit objects, we can generate predicted risk scores/ predicted probabilities of `Survived` in the test set.

```{r}
pred_prob1 <- predict(model1, newdata = test_set, type = "response")
pred_prob2 <- predict(model2, newdata = test_set, type = "response")
```

It is often useful to first get an idea of prediction performance independent of specific classification thresholds. Use the `pROC` (or `PRROC`) package to create roc objects for both risk score vectors.

```{r}
roc1 <- roc(test_set$Survived, pred_prob1)
roc2 <- roc(test_set$Survived, pred_prob2)
```

Now, you can print and plot the resulting `roc` objects.

```{r}
plot(roc1, col = "blue")
plot(roc2, col = "red", add = TRUE)
legend("bottomright", legend = c("Model 1", "Model 2"), 
       col = c("blue", "red"), lwd = 2)
#To check the exact Area under the curve values
auc(roc1)
auc(roc2)
```

In your own words, how would you interpret these ROC curves? What do you think about the ROC-AUCs we observe here?

#### Start text...
Looking at these ROC curves, I can see that both models demonstrate strong predictive performance for Titanic survival, with curves well above the diagonal line and AUC values around 0.80-0.85. The blue line (Model 1) and red line (Model 2) follow very similar paths, though Model 2 shows slightly better performance in certain regions. This tells me that adding the interaction term in Model 2 only marginally improved the model's predictive ability.

What I find particularly interesting is how both models maintain a good balance between sensitivity and specificity, especially in the crucial lower false positive rate regions. While Model 2 is technically more complex with its interaction terms, I don't see enough improvement in its performance to justify the added complexity. In practical terms, I'd probably lean towards using the simpler Model 1 since it achieves nearly equivalent results with a more straightforward structure.
#### end

As a next step, we want to predict class membership given the risk scores of our two models. Here we use the default classification threshold, 0.5.

```{r}
pred_class1 <- ifelse(pred_prob1 > 0.5, 1, 0)
pred_class2 <- ifelse(pred_prob2 > 0.5, 1, 0)
```

On this basis, we can use `confusionMatrix()` to get some performance measures for the predicted classes.

```{r}
conf_mat1 <- confusionMatrix(factor(pred_class1), test_set$Survived)
conf_mat2 <- confusionMatrix(factor(pred_class2), test_set$Survived)
conf_mat1
conf_mat2
```

Briefly explain potential limitations when measuring prediction performance as carried out in the last two code chunks.

#### Start text...
When I look at how we're measuring prediction performance in these code chunks, I notice several important limitations. The most obvious one is our use of a fixed 0.5 threshold for classification, which feels somewhat arbitrary. In real-world scenarios, especially with imbalanced datasets like the Titanic (where fewer people survived than died), this threshold might not be optimal. Additionally, we're relying on a single train/test split, which makes our performance metrics potentially sensitive to how we happened to split the data.

Another concern I have is about the depth of our evaluation. While confusion matrices provide useful metrics like accuracy and sensitivity, they don't tell us about the uncertainty in our predictions or how well calibrated our probability estimates are. The binary classification approach (survived/didn't survive) might also be oversimplifying what could be more nuanced probabilities of survival. Finally, I think it's worth noting that our test set size is relatively small, which means our performance metrics might have high variance and might not reliably generalize to new data.
#### end
